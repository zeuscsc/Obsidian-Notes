**Open-Unmix**, is a deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists. **Open-Unmix** provides ready-to-use models that allow users to separate pop music into four stems: **vocals**, **drums**, **bass** and the remaining **other** instruments.
Although **open-unmix** reaches state of the art separation performance as of September, 2019 (See [Evaluation](https://sigsep.github.io/open-unmix/#Evaluation)), the design choices for it favored simplicity over performance to promote clearness of the code and to have it serve as a **baseline** for future research. The results are comparable/better to those of `UHL1`/`UHL2` which obtained the best performance over all systems trained on MUSDB18 in the [SiSEC 2018 Evaluation campaign (opens new window)](https://sisec18.unmix.app/). We designed the code to allow researchers to reproduce existing results, quickly develop new architectures and add own user data for training and testing. We favored framework specifics implementations instead of having a monolithic repository with common code for all frameworks.

The model is available for three different frameworks. However, the pytorch implementation serves as the reference version that includes pre-trained networks trained on the [MUSDB18 (opens new window)](https://sigsep.github.io/datasets/musdb.html)dataset.