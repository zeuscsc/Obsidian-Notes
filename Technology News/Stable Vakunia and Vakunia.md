![[snapshots/QeBmeHg8s5Y/00-15-04.png]]
# Stable Vakunia and Vakunia
![[snapshots/QeBmeHg8s5Y/00-06-17.png]]
## Introduction to Stable Vakunia
![[snapshots/QeBmeHg8s5Y/00-01-17.png]]
Stable Vakunia is a fine-tuned version of the Vakunia 13 billion parameter model which was trained using reinforcement learning from human feedback. It was fine-tuned using three datasets and is expected to generate better results than the original model. However, there are concerns that RLHF models can sometimes perform worse than the base model.

![[snapshots/QeBmeHg8s5Y/00-04-14.png]]
## How to Install Stable Vakunia
To install Stable Vakunia in the UBAWUGA Tech Generation web UI, users need to download a 4-bit quantized model created by TheBlock. Once downloaded, the model needs to be saved in the models folder and its settings should be set to Wbits = 4 and group size = 128. It is also important to use Vakunia V0 instruction template to avoid the bug of the model talking to itself.

![[snapshots/QeBmeHg8s5Y/00-00-54.png]]
## Comparing Stable Vakunia Against Vakunia
![[snapshots/QeBmeHg8s5Y/00-02-40.png]]
In a series of tests, Stable Vakunia was compared against the base Vakunia 13 billion parameter model. Stable Vakunia performed better than Vakunia in simple general knowledge questions, creative writing prompts, and summarizing short articles. However, further tests are necessary to determine if Stable Vakunia is a better overall model.

![[snapshots/QeBmeHg8s5Y/00-08-58.png]]
## GPT-4's Opinion on the Results
![[snapshots/QeBmeHg8s5Y/00-06-44.png]]
The ratings of the generated responses were also evaluated using GPT-4 on a scale from 1 to 10. In most cases, GPT-4 rated Stable Vakunia higher than Vakunia. Stable Vakunia was often praised for being better organized and more comprehensive in its responses. However, it is important to note that these ratings are subjective and additional testing is needed to confirm the superiority of Stable Vakunia.## Testing Stable Vakunya against Vakunya
![[snapshots/QeBmeHg8s5Y/00-22-09.png]]
The video compares Stable Vakunya, a new language model, and Vakunya, a previously used language model, in solving various tasks like language translation, logical and coding problems. The video mentions Vakunya being the king of the 13 billion parameters model, and Stable Vakunya challenging its prowess.

![[snapshots/QeBmeHg8s5Y/00-12-20.png]]
## Translation Task
![[snapshots/QeBmeHg8s5Y/00-21-36.png]]
The video demonstrates Stable Vakunya and Vakunya translations from English to French. Stable Vakunya is better in comparison to Vakunya.

![[snapshots/QeBmeHg8s5Y/00-13-24.png]]
## Logical Puzzle
The video asks both Vakunya and Stable Vakunya to solve a simple puzzle, but Stable Vakunya could not provide the correct answer, while Vakunya gave a wrong answer. 

![[snapshots/QeBmeHg8s5Y/00-15-14.png]]
## Math and Coding Problems
![[snapshots/QeBmeHg8s5Y/00-21-41.png]]
The video measures Stable Vakunya and Vakunya's competency in solving math and coding problems. Both models end up producing incorrect solutions.

![[snapshots/QeBmeHg8s5Y/00-22-41.png]]
## Uncensored Content
![[snapshots/QeBmeHg8s5Y/00-20-32.png]]
The video asks Stable Vakunya to provide detailed instructions on how to build something that cannot be mentioned on YouTube, but neither models could answer it.

Source: [(95) StableVicuna is NOW the UNSTOPPABLE 13B LLM KING! Bye Vicuna! - YouTube](https://www.youtube.com/watch?v=QeBmeHg8s5Y)
